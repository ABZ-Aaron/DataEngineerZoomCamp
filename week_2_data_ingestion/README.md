# Week 2

The goal of this week is to orchestrate a job to ingest web data to a Data Lake in it's raw form.

## Data Lakes

### What is a Data Lake?

A Data Lake is a repository which holds Big Data from many sources, which include structured, semi-structured, and unstructured data.

* Ingest Unstructured and Structured data
* Stores, secures, and protects data at unlimited scales
* Connects data with analytics and machine learning tools
* Allows us to store and access data quickly, without thinking about schemas

They contain huge amounts of data, sometimes ingesting data everyday. In Data Warehousing, data is usually structured, and data size is generally small.

### ELT vs ETL

**Extract, Load and Transform** vs. **Extract, Transform and Load**.

ELT provides data lake support (schema on read). It is used for large amounts of data.

### Problems

Data lakes can turn into "data swamps". This happens due to no versioning, incompatible schemas for same data without versioning, no metadata associated, and joins are not possible. Basically, it can become a bit of a mess!

### Providers

Cloud providers provide data lakes solutions. 

* Azure - AZURE BLOB
* AWS - S3
* GCP - Cloud Storage

## Workflow Orchestration

A data pipelines can be generally defined as sript(s) that take in data, do something to the data, and output it somewhere. This is what our pipeline looked like last week:

<img src="https://github.com/ABZ-Aaron/DataEngineerZoomCamp/blob/master/images/pipeline.png" width=100% height=40%>

[source](https://youtu.be/0yK7LXwYeD0)

This week we will work on a slightly more complex pipeline. This will involve extracting data from the web with `wget` or something else to produce a CSV which we store locally. We will convert this CSV to a more effective format - parquet. We'll take this file and upload to Google Cloud Storage (data lake). We will then copy this to Google Biq Query (data warehouse).

We need to make sure we run these in the right order. 

<img src="https://github.com/ABZ-Aaron/DataEngineerZoomCamp/blob/master/images/workflow.png" width=100% height=40%>

[source](https://youtu.be/0yK7LXwYeD0)

This is called a data workflow, or sometimes known as a DAG (directed acyclic graph). The edges are the dependencies. So for example, parquet depends on the the `wget` stage. An example of the above with a cycle could be if we re-run the `wget` stage after uploading to GCS.

A parameter of the entire workflow could be month (e.g. 2021-01). 

But how to we manage this workflow / DAG? 

There are many tools:

* Luigi (not as popular nowadays)
* PREFECT
* Apache Airflow (this is what we will use, and is probably the most popular)

## Airflow

Apache Airflow is a platform to programmatically schedule and mointor workflows as DAGs. With Airflow, we have command line utilities as well as a user interface to visualise pipelines, monitor progress and troubleshoot issues.

Here's the general architecture:

<img src="https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/arch-diag-basic.png" width=100% height=40%>

[source](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/2_concepts.md)

* `Web Server` - GUI to inspect, trigger and debug behaviour of DAGS. Available at http://localhost:8080.

    The home page of the web server shows us a list of DAGs. The DAGs properties can be seen here (where the source file resides, tags, descriptions, and so on). The DAGs can also easily be paused here, which will then ignore any schedules you may have set. You can see the names of the DAGs, the schedule that they run on (in [CRON](https://crontab.guru/#5_4_8_*_*) format), the owner of the DAG, recent tasks, a timestamp of the last run of the DAG, summary of previous DAGs run, and so on.

    You can also view the DAG as a graph, after going to the DAG detail page. We can also view the code behind the DAG here as well.

* `Scheduler` - Responsible for scheduling jobs.

    This constantly monitors DAGs and taks and running any that are scheduled to run and have had their dependencies met.

* `Worker` - Executes the tasks given by scheduler.
* `MetaData Database` - Backend to Airflow. Used by scheduler and executor and webserver to store data.

    This contains all the metadata related to the execution history of each task and DAG as well as airflow configuration. I believe the default in SQLite, but can easily be configured to PostgreSQL or some other database system. The database is created when we initialise using `airflow-init`. Information in this database includes task history.

* `redis` - Forwards messages from scheduler to worker
* `flower`- Flower app for monitoring the environment. Available at http://localhost:5555.
* `airflow-inti` - Initialises service

If we're running Airflow in Docker, we use something called `CeleryExecutor`

### What is an Executor and what is CeleryExecutor?

Once we define a DAG, the following needs to happen in order for a single set of tasks within that DAG to execute and complete from start to finish:

1. The `Metadata Database` keeps a record of all tasks within the DAG, along with their status (e.g. failed, running, scheduled).

2. The `Scheduler` reads from the `Metadata Database` to check the status of each tasks and decide what needs done.

3. The `Executor` works with the `Scheduler` to determine what resources will complete those tasks as they're queued. In other words, it runs tasks taht the `Scheduler` determines are ready to run. The SequentialExecutor is the default. This can only run one task at a time, and is not meant for production. It's really just for testing simple DAGs. However, it's the only one that is currently compatible with SQLite. We're using PostgreSQL, and we want to run more complex DAGs, wo we're going with `CeleryExecutor` which is built for horizontal scaling, or distributed computing. This works with pools of independent `workders` across which it can delegate tasks.

### Additional Terminology

* `Operators` - Each task implements an **operator**. These are what actually execute scripts, commands, and so on. These include `PythonOperator`, `BashOperator` and `PostgresOperator`. These are assigned to each task/node in a DAG.

### Running Airflow with Docker

There's a few steps required to get Airflow working with Docker:

1. Install [Docker Community Edition](https://docs.docker.com/engine/installation/) on your local machine.
2. Configure Docker instance to use 4GB of memory.
3. Install [Dockder Compose](https://docs.docker.com/compose/install/).

We then fetch `docker-compose.yaml` which uses the latest airflow image. To do so, run:

`curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.2.3/docker-compose.yaml'`

Within this file, there are several definitions:

* `airflow-scheduler` - This monitors all tasks & DAGs.
* `airflow-webserver` - Available at http://localhost:8080
* `airflow-worker` - This executes the tasks given by scheduler
* `airflow-init` - Initialises services
* `flower` - Monitors oru environment
* `postgres` - The database
* `redis` - broker and forwards messages from scheduler to worker

With these services, we are able to run Airflow with `CeleryExecutor`.

Some of these services are mounted, meaning their contents are synced between our local machine and the container:

* `/dags` - DAG files go here
* `/logs` - Logs from task execution & scheduler
* `/plugins` - Custom plugins go here

Some workflow components:

* DAG - Specifies the dependencies between a set of tasks with explicit execution order
* Tasks - A definied unit of work. These define what to do (e.g. running analysis)
* DAG Run - Individual execution of a DAG
* Task Instance - Individual run of a single Task. Each task has a state (failed, success, etc). Ideally, they run from:

`none` > `scheduled` > `queued` > `running` > `success`

### Prerequisites for Airflow & Week 2

Here is some initial setup required for week 2 of this zoomcamp:

1. Rename our Google Cloud Platform Service Account Credentials JSON file to `google_credentials.json`. Store it in our home directory.

```bash
cd ~ && mkdir -p ~/.google/credentials/
mv <path to JSON file> ~/.google/credentials/google_credentials.json
```

2. Upgrade `docker-compose` to version 2.x+ if required. 

3. Set memory of Docker Engine to 4-8GB. This can be done in Docker Desktop.

4. Make sure you have Python version 3.7+ installed. Find python version with `python --version`

4. Create subdirectory called `airflow` in `project` directory.

5. Import official image and setup for latest Airflow version. I mentioned this earlier:

```bash
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.2.3/docker-compose.yaml'
```

6. This might only be required on linux. The Quick Start needs to know our host user id and needs to have group id set to 0. This ensure files creates in `dags`, `logs` and `plugins` will be created with root user:

```bash
mkdir -p ./dags ./logs ./plugins
echo -e "AIRFLOW_UID=$(id -u)" > .env
```

7. If you want to run Airflow locally, we might want to use an extended image with additional dependencies such as python packages. 

8. Create a `Dockerfile` pointing to Airflow version we've just downloaded (e.g. apache/airflow:2.2.3). In the `Dockerfile` add custom packages to be installed - `glcoud`. This will allow us to connect with the GCS bucket/data lake. Also integrate `requirements.txt` which will be used to install libraries via `pip install`. Here we specify `apache-airflow-providers-google`. This is a google specific client for airflow. We also specified `pyarrow`. This is used for conversion of our data to parquet packets.

9. In Docker Compose YAML file, under `x-airflow-common`, remove the image tag to replace with our build from the Dockerfile. Mount `google_credentials` in `volumes` section as read-only. Set environment variables `GOOGLE_APPLICATION_CREDENTIALS` AND `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT`. Change `AIRFLOW__CORE__LOAD_EXAMPLES` to false

For the last steps, you can copy the `Dockerfile` and `docker-compose.yml` from the [zoomcamp repo](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_2_data_ingestion/airflow).

In the `docker-compose.yml` file, be sure to put the correct details into these two variables:

```bash
GCP_PROJECT_ID: "<GCP Project ID>"
GCP_GCS_BUCKET: "<ID of Bucket Created in GCP>"
```

10. Some additional points.

In the `docker-compose.yaml` file, this was added:

```bash
  build:
    context: .
    dockerfile: ./Dockerfile
```

This replaced the following:

```bash
image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.2.3}
```

For our Airflow environment to interact with the Google Cloud Platform environment, we need to build a custom `Dockerfile` and use the base image found in the docker-compose file (removing the base image from the docker-compose file). Then install all the related GCP requirements. This will install `gcloud` and sets it into our path, as well as a few other things.

## Running Airflow

1. With the pre-requisites out of the way, we now need to build the docker image (we run this the first time, and any time we update our `Dockerfile`).

```bash
docker-compose build
```

From my understanding, this will read our `docker-compose.yaml` file, looks for all services containing `build:` and run `docker build` on them. In our case, this is just our `Dockerfile`.

2. Initialise the Airflow scheduler, database, and other configurations.

```bash
docker-compose up airflow-init
```

3. Kick off all services

```bash
docker-compose up
```

This might take a bit of time. 

4. Once complete, navigate to `localhost:8080` and input our default credentials. These are `airflow / airflow`.

5. We can close our containers using:

```bash
docker-compose down
```

### Simple Tutorial

*This is a tutorial taken from [Data Pipelines Pocket Reference](https://www.amazon.co.uk/Data-Pipelines-Pocket-Reference-Processing/dp/1492087831)*

Remember, DAGs are defined in Python scripts - where its structure and dependences are defined. 

1. First, in your `airflow/dags` folder, create a python file called `simple_dag.py` and populate it with the following:

    ```python
    from datetime import timedelta
    from airflow import DAG 
    from airflow.operators.bash_operator import BashOperator
    from airflow.utils.dates import days_ago

    dag = DAG(
        'simple_dag',
        description = 'A Simple DAG',
        schedule_interval = timedelta(days = 1),
        start_date = days_ago(1),
    )

    t1 = BashOperator(
        task_id = 'print_date',
        bash_command = 'date',
        dag = dag,
    )

    t2 = BashOperator(
        task_id = 'sleep',
        depends_on_past = False,
        bash_command = 'sleep 3',
        dag = dag,
    )

    t3 = BashOperator(task_id = 'print_end', depends_on_past = False, bash_command = 'echo \'end\'', dag = dag)

    t1 >> t2
    t2 >> t3
    ```


    Airflow will scan this folder periodically for DAG files, which you will then see in the Airflow Webserver UI. You may have to give it a few mins, or restart your webserver.

    You can see it's just a typical python script with imports and so on. The first block of code is where we've defined a DAG called `simple_dag`. We've given it a schedule and start date. 

    Next, three tasks have been defined all of type `BashOperator`. This means that when they are executed, they will run a bash command. 

    The last two lines define dependencies. Here we can see that when `t1` completes, `t2` starts. And when `t2` completes, `t3` starts. 

2. At this point. I'd recommend looking at the web UI for Airflow and playing around with it. Click on the DAG, see what the tree and graph look like, see the code you just wrote in the UI itself, and so on. Get familiar with it.

3. Next, flip the toggle on the DAG page, or in the detailed view of the DAG, to turn it on. In the code, we used `tiemdelta(days =1)`. This basically means the DAG will run once a day at midnight. We can see this schedule on the home (DAG) page, and on the DAG detail page, calendar etc. In this example, start_date was set one day prior, meaning that as soon as it's enabled, it would run. 

4. If you go to Browse > DAG Runs, you can see a visual status of the DAG run. By the time you've read this and checked, it will liked say `success`. If you want to run the DAG manually, you can do so from the DAG detail page (look for a play like symbol. You can also delete the DAG here).

5. Now, this DAG doesn't exactly do a lot. It just runs a couple bash commands. If you want to see what was printed from the bash commands, you can actually view this in the logs. This can be useful to troubleshoot and make sure that the commands worked as expected. You can check your log by going to the DAG detail view. Then clicking on one of the graph nodes on the `Graph` tab (e.g. print_date). From there, click on `Log`. If you scroll to the bottom of the log, you should see the output of the command under where it says `Output`. You'll also see `Command exited with return code 0`. This is good. Any value differenrt that 0 means there was an error. 

6. Now just play around. To create a more complex DAG, it might use bash commands to execute a couple of python scripts. Those python scripts might extract CSV data from a website, then send that data to a databsae, or a bucket in the cloud. When these tasks complete, you could have the next task be to load that data into a data warehouse somewhere. 

    But why use `BashOperator` to execute python code insteaad of `PythonOperator`? Arguably, it's easier to maintain logic across the data infrastructure. If you use the `PythonOperator` the code must be written in the DAG definition file or imported into it. This means there's not much seperation between the actual orchestration and the logic of the process it executes. It also helps us avoid issues with incompatible versinos of python libraries and airflow. But... you can use `PythonOperator` if you want.

7. What if we ant to setup Alerts and Notifications? We don't necessarily want to be logging into the Airflow Webserver all the time to check stuff. We can instead have airflow send us or someone else an email when there's been a success or failure. This invovles providing SMTP server details to the airflow.cfg file, so I won't do this right now. Just know that it's possible. It really just a case of adding a bit more code to the DAG python file. 

    That's all for now. There's a lot more to cover, but hopefully this helps!


### Ingesting Data to GCP with Airflow

To get started, create a DAG (Py file) and store in the `dags` folder.

In the GCS python file, be sure to amend this line if necessary:

`BIGQUERY_DATASET = os.environ.get("BIGQUERY_DATASET", 'trips_data_all"')`

You may not need to change anything. But `trips_data_all` should represent the name of your big query dataset, which you will find in the `variables` file we defined in week 1 under the `terraform` folder.

I've commented the below file to try explain things:

```python
# data_ingestion_gcs_dag.py

import os
import logging

from airflow import DAG
from airflow.utils.dates import days_ago

# Import our operators
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

# We installed the following from the requirements file which was specified in the Dockerfile.

#Â Helps to interact with Google Storage
from google.cloud import storage

# Helps interact with BigQuery
from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator

# Helps convert our data to parquet format
import pyarrow.csv as pv
import pyarrow.parquet as pq

# Set some local variables based on environmental varaibles we specified in docker-compose.yaml
PROJECT_ID = os.environ.get("GCP_PROJECT_ID")
BUCKET = os.environ.get("GCP_GCS_BUCKET")

# Specify our dataset
dataset_file = "yellow_tripdata_2021-01.csv"
dataset_url = f"https://s3.amazonaws.com/nyc-tlc/trip+data/{dataset_file}"

# Store environmental variables (in your docker container) locally. The second argument of each `.get` is what it will default to if it's empty.
path_to_local_home = os.environ.get("AIRFLOW_HOME", "/opt/airflow/")
BIGQUERY_DATASET = os.environ.get("BIGQUERY_DATASET", 'trips_data_all')

# Replace CSV with Parquet on our file
parquet_file = dataset_file.replace('.csv', '.parquet')

def format_to_parquet(src_file):
    """Takes our source file and converts it to parquet"""
    if not src_file.endswith('.csv'):
        logging.error("Can only accept source files in CSV format, for the moment")
        return
    table = pv.read_csv(src_file)
    pq.write_table(table, src_file.replace('.csv', '.parquet'))


# NOTE: takes 20 mins, at an upload speed of 800kbps. Faster if your internet has a better upload speed
def upload_to_gcs(bucket, object_name, local_file):
    """
    Ref: https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python
    :param bucket: GCS bucket name
    :param object_name: target path & file-name
    :param local_file: source path & file-name
    :return:
    """
    # WORKAROUND to prevent timeout for files > 6 MB on 800 kbps upload speed.
    # (Ref: https://github.com/googleapis/python-storage/issues/74)
    storage.blob._MAX_MULTIPART_SIZE = 5 * 1024 * 1024  # 5 MB
    storage.blob._DEFAULT_CHUNKSIZE = 5 * 1024 * 1024  # 5 MB
    # End of Workaround

    client = storage.Client()
    bucket = client.bucket(bucket)

    blob = bucket.blob(object_name)
    blob.upload_from_filename(local_file)

default_args = {
    "owner": "airflow",
    "start_date": days_ago(1),
    "depends_on_past": False,
    "retries": 1,
}

# NOTE: DAG declaration - using a Context Manager (an implicit way)
with DAG(
    dag_id="data_ingestion_gcs_dag",
    schedule_interval="@daily",
    default_args=default_args,
    catchup=False,
    max_active_runs=1,
    tags=['dtc-de'],
) as dag:

    download_dataset_task = BashOperator(
        task_id="download_dataset_task",
        bash_command=f"curl -sS {dataset_url} > {path_to_local_home}/{dataset_file}"
    )

    format_to_parquet_task = PythonOperator(
        task_id="format_to_parquet_task",
        python_callable=format_to_parquet,
        op_kwargs={
            "src_file": f"{path_to_local_home}/{dataset_file}",
        },
    )

    # TODO: Homework - research and try XCOM to communicate output values between 2 tasks/operators
    local_to_gcs_task = PythonOperator(
        task_id="local_to_gcs_task",
        python_callable=upload_to_gcs,
        op_kwargs={
            "bucket": BUCKET,
            "object_name": f"raw/{parquet_file}",
            "local_file": f"{path_to_local_home}/{parquet_file}",
        },
    )

    bigquery_external_table_task = BigQueryCreateExternalTableOperator(
        task_id="bigquery_external_table_task",
        table_resource={
            "tableReference": {
                "projectId": PROJECT_ID,
                "datasetId": BIGQUERY_DATASET,
                "tableId": "external_table",
            },
            "externalDataConfiguration": {
                "sourceFormat": "PARQUET",
                "sourceUris": [f"gs://{BUCKET}/raw/{parquet_file}"],
            },
        },
    )

    download_dataset_task >> format_to_parquet_task >> local_to_gcs_task >> bigquery_external_table_task
```

For the above file, you'll see it in Airflow. Try running it. Basically, it will download our dataset, convert it to parquet format, upload the data to our GCP storage bucket, then transferring that to BigQuery.

---

**ISSUES / PROBLEMS**

I Haven't worked this out yet, but I was getting issues with the last two tasks. I checked the log files and identified the problem - although I don't know how it came about. I appeared that my Service Account in GCP didn't have the right Roles assigned (remember the ones we assigned in Week 1). I went back into GCP and assigned these. After that, it all worked fine. Still need to investigate what's going on here.

## Moving files from AWS to GPC with Transfer Service

`Google Transfer Service` allows us to pull data from a variety of different sources to our Google Cloud Storage, and move data between these. 

To initialise a job, we can use the UI, or we can use Terraform. 

### UI Method

1. Choose source packets (e.g. Amazon S3)
    * provide access keys (access key id and secret access key - at least if working with AWS)
2. Choose destination or bucket to store our data
3. Choose settings
    * Do we want to overwrite data, or delete data?
    * Name transfer job
4. Scheduling options
    * When do we want this to run? Every day?

And that's it!

If we go inside the job, we can view some details about it, pause the run, cancel it, check its progress, have fast its transferring, and so on.

BUT....

Google Transfer does cost money. You pay on a GB basis. This can be a cheaper option that spending a lot of time developing an Airflow DAG. But, if you're running it regularly, you might be better building a transfer service on your own using something like Airflow.

### Terraform Method

I'm not going to spend a lot of time understanding Terraform right now, but click [here](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/storage_transfer_job) for an example usage (nightly transfer job from an AWS S3 bucket to a GCS bucket). It's not too difficult to understand what the code is doing at each step.

If we wanted to configue something like this, we would need to setup a transfer_service.tf file within our terraform directory and run the usual commands `terraform plan`, `terraform apply` etc. You can see more details of this by checking out the terraform branch from the Zoomcamp main GitHub repository.

